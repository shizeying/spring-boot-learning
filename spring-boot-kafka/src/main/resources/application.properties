server.port=9999
#生产者
spring.kafka.bootstrap-servers=192.168.110.110:9092
# 每次批量发送消息的数量
spring.kafka.producer.retries=1
#序列化
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#序列化
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#用于控制发送记录在服务端的持久化
spring.kafka.producer.acks=-1
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id=1
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
spring.kafka.producer.compression-type=none
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id=test
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest
spring.kafka.consumer.auto-offset-reset=earliest
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode=MANUAL_IMMEDIATE
kafka.topic.elasticsearch-to-mongo=test3
kafka.topic.mongo-to-elasticsearch=test4
