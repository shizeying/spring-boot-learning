server.port                                = 9999
#生产者
spring.kafka.bootstrap-servers             = 192.168.110.110:9092
spring.kafka.zookeeper-connects            = 192.168.110.110:2181
#用于控制发送记录在服务端的持久化
spring.kafka.producer.acks                 = -1
#ID在发出请求时传递给服务器，用于服务器端日志记录
spring.kafka.producer.client-id            = 1
#生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'），
spring.kafka.producer.compression-type     = none
#用于标识此使用者所属的使用者组的唯一字符串。
spring.kafka.consumer.group-id             = test
#当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时该怎么办，默认值为latest
spring.kafka.consumer.auto-offset-reset    = earliest
#如果为true，则消费者的偏移量将在后台定期提交，默认值为true
spring.kafka.consumer.enable-auto-commit   = false
spring.kafka.consumer.key-deserializer     = org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer   = org.apache.kafka.common.serialization.StringDeserializer
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.kafka.listener.ack-mode             = MANUAL_IMMEDIATE
spring.kafka.topic                         = zhangsan
spring.kafka.consumer.auto-commit-interval = 1
#当enable.auto.commit的值设置为false时，该值会生效；为true时不会生效
spring.elasticsearch.rest.password         =
spring.elasticsearch.rest.username         =
spring.elasticsearch.rest.uris             = http://localhost:9200
spring.kafka.elasticsearch-index           = a
spring.kafka.elasticsearch-index-bulk      = 100



